{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean text saved in clean_text\\clean_text_Criminal Justice and Police Act 2001 (c. 16).csv.\n",
      "Clean text saved in clean_text\\clean_text_Human Rights Act 1998 (c. 42).csv.\n",
      "Clean text saved in clean_text\\clean_text_Public Order Act 1986 (c. 64).csv.\n",
      "Clean text saved in clean_text\\clean_text_Public Order Act 2023 (c. 15).csv.\n",
      "Clean text saved in clean_text\\clean_text_Serious Organised Crime and Police Act 2005 (c. 15).csv.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "def read_html_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "    return lines\n",
    "\n",
    "folder_path = 'raw_html'  # Specify the folder path containing the HTML files\n",
    "\n",
    "output_folder = 'clean_text'  # Specify the folder where the clean text files will be saved\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create the output folder if it doesn't exist\n",
    "\n",
    "html_files = os.listdir(folder_path)  # Get the list of HTML files in the folder\n",
    "\n",
    "for html_file in html_files:\n",
    "    html_file_path = os.path.join(folder_path, html_file)\n",
    "    lines = read_html_file(html_file_path)\n",
    "    html_text = ''.join(lines)  # Join the lines into a single string\n",
    "    clean_text = remove_html_tags(html_text)\n",
    "\n",
    "    # Extract the filename from the html_file_path\n",
    "    filename = os.path.splitext(html_file)[0]  # Extract the filename without extension\n",
    "    output_file_path = os.path.join(output_folder, f'clean_text_{filename}.csv')\n",
    "\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        for line in clean_text.splitlines():\n",
    "            writer.writerow([line])\n",
    "\n",
    "    print(f\"Clean text saved in {output_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Function to perform topic modeling on a list of tokens\n",
    "def perform_topic_modeling(tokens):\n",
    "    if not tokens:\n",
    "        return []  # Return empty topics if there are no tokens\n",
    "    dictionary = corpora.Dictionary([tokens])\n",
    "    corpus = [dictionary.doc2bow(tokens)]\n",
    "    lda_model = models.LdaModel(corpus, num_topics=1, id2word=dictionary)\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    return [topic[1] for topic in topics]\n",
    "\n",
    "# Read CSV file\n",
    "data = []\n",
    "with open('clean_text\\\\clean_text_Human Rights Act 1998 (c. 42).csv', 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    for row in reader:\n",
    "        text = row[0]  # Assuming the text is in the first column\n",
    "        tokens = preprocess_text(text)\n",
    "        topics = perform_topic_modeling(tokens)\n",
    "        data.append([text, topics])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data, columns=['Text', 'Topics'])\n",
    "\n",
    "# Save DataFrame to CSV file\n",
    "df.to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the search query without using word vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: damage\n",
      "\n",
      "Matching Results:\n",
      "                                                                                                                                          Text\n",
      "(2)But damages may be awarded only by a court which has power to award damages, or to order the payment of compensation, in civil proceedings.\n",
      "                                  (3)No award of damages is to be made unless, taking account of all the circumstances of the case, including—\n",
      "                                                                                                               (a)whether to award damages, or\n",
      "                                                                     (5)A public authority against which damages are awarded is to be treated—\n",
      "                                                                        “damages” means damages for an unlawful act of a public authority; and\n",
      "              [F16(3)In proceedings under this Act in respect of a judicial act done in good faith, damages may not be awarded otherwise than—\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the output.csv file into a DataFrame\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Function to search the DataFrame based on the query\n",
    "def search_dataframe(query):\n",
    "    # Filter rows that match the query (case-insensitive)\n",
    "    matching_rows = df[df['Topics'].str.contains(query, case=False)]\n",
    "    return matching_rows\n",
    "\n",
    "# Get the query from user input\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Search the DataFrame\n",
    "results = search_dataframe(query)\n",
    "\n",
    "# Print the query text in a pretty way\n",
    "print(f\"Query: {query}\\n\")\n",
    "\n",
    "# Display the matching results without the Topics column\n",
    "print(\"Matching Results:\")\n",
    "print(results.drop(columns=['Topics']).to_string(index=False))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the search using word vectarization with the google training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Load the output.csv file into a DataFrame\n",
    "df = pd.read_csv('output.csv')\n",
    "\n",
    "# Load the Word2Vec model (pre-trained or trained on your data)\n",
    "model_path = 'GoogleNews-vectors-negative300.bin'\n",
    "word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Query: Attack\n",
      "Expanded Query: Attack attacks attck atack assault atttack bombing assaults attacking attacked counterattack\n",
      "\n",
      "Matching Results:\n",
      "                                                                                                                                                                                                                                                                                                                         Text\n",
      "(7)“Overseas operations” means any operations outside the British Islands, including peacekeeping operations and operations for dealing with terrorism, civil unrest or serious public disorder, in the course of which members of Her Majesty's forces come under attack or face the threat of attack or violent resistance.\n"
     ]
    }
   ],
   "source": [
    "# Function to expand the query with similar words\n",
    "def expand_query(query):\n",
    "    expanded_query = []\n",
    "    for word in simple_preprocess(query):\n",
    "        similar_words = word2vec_model.most_similar(word)\n",
    "        expanded_query.extend([word] + [similar_word[0] for similar_word in similar_words])\n",
    "    return ' '.join(expanded_query)\n",
    "\n",
    "# Function to search the DataFrame based on the expanded query\n",
    "def search_dataframe(query):\n",
    "    # Split the query into individual words\n",
    "    words = query.split()\n",
    "    \n",
    "    # Initialize an empty list to store matching DataFrames\n",
    "    matching_dfs = []\n",
    "    \n",
    "    # Search for each word in the query\n",
    "    for word in words:\n",
    "        # Filter rows that match the word (case-insensitive)\n",
    "        matching_rows = df[df['Topics'].str.contains(word, case=False, regex=False)]\n",
    "        # Append the matching DataFrame to the list\n",
    "        matching_dfs.append(matching_rows)\n",
    "    \n",
    "    # Concatenate all the matching DataFrames into a single DataFrame\n",
    "    results = pd.concat(matching_dfs, ignore_index=True)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Get the query from user input\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "# Expand the query with similar words\n",
    "expanded_query = expand_query(query)\n",
    "\n",
    "# Search the DataFrame\n",
    "results = search_dataframe(expanded_query)\n",
    "\n",
    "# Print the original and expanded queries in a pretty format\n",
    "print(f\"Original Query: {query.capitalize()}\")\n",
    "print(f\"Expanded Query: {expanded_query.capitalize()}\\n\")\n",
    "\n",
    "# Display the matching results without the Topics column\n",
    "print(\"Matching Results:\")\n",
    "print(results.drop(columns=['Topics']).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
